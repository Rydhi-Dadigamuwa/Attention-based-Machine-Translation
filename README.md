# Attention-Based Neural Machine Translation 🌍🧠

This project implements a **Neural Machine Translation (NMT)** system using the **Bahdanau-style attention mechanism**. The model is designed to learn alignment between input and output sequences dynamically, leading to improved translation performance and interpretability.

---

## 📄 Inspired By

> **"Neural Machine Translation by Jointly Learning to Align and Translate"**  
> Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio  
> [arXiv:1409.0473](https://arxiv.org/abs/1409.0473)

---

## ⚙️ Architecture Overview

The model consists of the following main components:

- **Bidirectional LSTM Encoder** to process input sequences from both directions.
- **Bahdanau Attention Mechanism** to compute alignment between encoder outputs and decoder states.
- **LSTM Decoder** that uses the context vector to generate the translated sequence.

🖼️ **Model Architecture Diagram:**

<p align="center">
  <img src="https://github.com/Rydhi-Dadigamuwa/Attention-based-Machine-Translation/blob/main/model.png" alt="Model Architecture" width="600">
</p>

---

## 🧠 Key Features

- Implements **Bahdanau-style soft attention** from scratch
- Learns to **align** and **translate** simultaneously
- **Visualizes attention maps** to interpret model behavior
- Built using **Keras** and **TensorFlow**

---

## 📸 Attention Map Visualizations

Below are examples of the attention maps generated by the model. These show how the decoder attends to different parts of the input sequence when predicting each word in the translated sentence:

### 🔍 Example 1:
<p align="center">
  <img src="https://github.com/Rydhi-Dadigamuwa/Attention-based-Machine-Translation/blob/main/Attention_plot_example_1.png" alt="Attention Map Example 1" width="700">
</p>

### 🔍 Example 2:
<p align="center">
  <img src="https://github.com/Rydhi-Dadigamuwa/Attention-based-Machine-Translation/blob/main/Attention_plot_example_2.png" alt="Attention Map Example 2" width="700">
</p>

---
